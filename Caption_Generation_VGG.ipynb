{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Caption Generation using VGG\n",
    "MD Muhaimin Rahman\n",
    "contact: sezan92[at]gmail[dot]com\n",
    "\n",
    "In this project, I have tried to work on Caption generation of Images of Flickr_8k dataset. I took extensive help from Jason Brownlee's Blog [article](https://machinelearningmastery.com/develop-a-deep-learning-caption-generation-model-in-python/) on the same dataset. But I thought some codeblocks were unnecessarily complex . So I changed them for my project. The main architecture is mainly taken from Googles [paper](https://arxiv.org/abs/1411.4555),"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Organization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make sense of this code, we need to get an idea about how the dataset is organized. I have used Flickr8k dataset, which I cannot redistribute. You have to fillup this [form](https://forms.illinois.edu/sec/1713398) and they will give you the dataset. You have to keep the folders ```Flicker8k_Dataset``` and ```Flickr_Text``` inside the ```dataset``` folder. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ```Flicker8k_Dataset``` has all the images - train,test,validation- all of them . The ```Flickr_Text``` folder has some ```txt``` file , We will need four text files\n",
    "* Flickr8k.token.txt\n",
    "* Flickr_8k.trainImages.txt\n",
    "* Flickr_8k.devImages.txt\n",
    "* Flickr_8k.testImages.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``` Flickr8k.token.txt``` contains 4 captions for every image name . ```Flickr_8k.trainImages.txt``` contains the names of train images ,```Flickr_8k.devImages.txt``` and ```Flickr_8k.testImages.txt``` contain validation and test image names consequently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The job of Data preprocessing here is as following \n",
    "* Extract features from every image and save them in a pickle file\n",
    "* Extract captions for every image from ```Flickr8k.token.txt``` file and save them as dictionary\n",
    "* Separate image names with their captions for every dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import pickle\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.applications import Xception,InceptionV3,InceptionResNetV2\n",
    "from keras.preprocessing.image import load_img\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "from keras.utils import np_utils\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import plot_model\n",
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Dropout\n",
    "from keras.layers.merge import add\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to load the VGG16 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prev_model():\n",
    "    model= VGG16()\n",
    "    model.layers.pop()\n",
    "    model = Model(inputs=model.inputs, outputs=model.layers[-1].output)\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Directory of the Flicker8k_Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = 'dataset/Flicker8k_Dataset'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function for extracting Features. \n",
    "* if the features already available it will return the extracted features file\n",
    "* else Extract features from every image\n",
    "* Save the Extracted features of All images in one pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(directory,model_name):\n",
    "    if os.path.exists('dataset/features_%s.pkl'%(model_name)):\n",
    "        print(\"Features file already exists\")\n",
    "        features = pickle.load(open('dataset/features_%s.pkl'%(model_name)))\n",
    "        return features\n",
    "    else:\n",
    "        model = prev_model()\n",
    "        image_names = os.listdir(directory) \n",
    "        features = dict()\n",
    "        for image_name in image_names:\n",
    "            image =load_img(directory+'/'+image_name,target_size=(224,224))\n",
    "            image = img_to_array(image)\n",
    "            image = preprocess_input(image)\n",
    "            image = image.reshape((1,image.shape[0],image.shape[1],image.shape[2]))\n",
    "            feature = model.predict(image)\n",
    "            features[image_name] = feature\n",
    "\n",
    "            print('%s done!'%(image_name))\n",
    "        pickle.dump(features,open('dataset/features_%s.pkl'%(model_name),'w')) \n",
    "        return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = extract_features(directory,'VGG16')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A single Feature vector dimension. Should be (4096,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function for Extracting captions from caption text files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_captions(filename):\n",
    "    if os.path.exists('dataset/data_with_captions.pkl'):\n",
    "        print('Data with Captions file already exists')\n",
    "        dataset = pickle.load(open('dataset/data_with_captions.pkl'))\n",
    "        return dataset\n",
    "    else:\n",
    "        text = open(filename).read()\n",
    "        text = text.split(('\\n'))\n",
    "        captions=[]\n",
    "        image_names=[]\n",
    "        dataset=dict()\n",
    "        for line in text:\n",
    "            if len(line)<1:\n",
    "                break\n",
    "            #print(line.split('\\t')[0].split('#')[0])\n",
    "            caption = line.split('\\t')[1]\n",
    "            image_name = line.split('\\t')[0].split('#')[0]\n",
    "            if image_name not in dataset:\n",
    "                dataset[image_name] =[]\n",
    "            else:\n",
    "                dataset[image_name].append(caption.lower())\n",
    "                #dataset[image_name] =' '.join(dataset[image_name])\n",
    "            print(image_name+\" done!\")\n",
    "        pickle.dump(dataset,open('dataset/data_with_captions.pkl','w'))\n",
    "\n",
    "        return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename= 'dataset/Flickr_Text/Flickr8k.token.txt'\n",
    "\n",
    "data_with_captions= extract_captions(filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_filename = 'dataset/Flickr_Text/Flickr_8k.trainImages.txt' \n",
    "dev_filename='dataset/Flickr_Text/Flickr_8k.devImages.txt'\n",
    "test_filename = 'dataset/Flickr_Text/Flickr_8k.testImages.txt'\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function for loading dataset. That is, making list of names of each imageset folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(filename):\n",
    "    image_names = open(filename).read().split('\\n')\n",
    "    for image_name in image_names:\n",
    "        if len(image_name)<1:\n",
    "            image_names.remove(image_name)\n",
    "    return image_names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = load_dataset(train_filename)\n",
    "dev= load_dataset(dev_filename)\n",
    "test=load_dataset(test_filename)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting features for every dataset. That is, to extract features for images of every dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features(dataset):\n",
    "    features_dict= dict()\n",
    "    features = pickle.load(open('features_VGG16.pkl'))\n",
    "    for image_name in dataset:\n",
    "        features_dict[image_name]= features[image_name]\n",
    "    return features_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features_set = get_features(train)\n",
    "test_features_set = get_features(test)\n",
    "dev_features_set = get_features(dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now Stop! Take a deep breath, and start again!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Previous steps were just the starting. Now what comes is a bit tough. Please go slowly , and try to understand how it will work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The architecture will work in a different way. First, have a look at the following image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![SkateBoard](Caption0.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What the model will do, is it will take the image feature and a trigger word ,  $start$ . Then it will predict the next word, which is in our case $a$ . Then it will merge the feature with trigger word and the first predicted word . Then, it will predict second word , which is in our case $skateborder$ . It will continue to do so until it reaches the final trigger, which will be in our case $end$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please have a look at the following flow chart"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![FlowChart](FlowChart2.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### So we , need to process the dataset , again!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So to train the Model , we need data like this\n",
    "* Feature + 'start' , Prediction 'a'\n",
    "* Feature + 'start a' , Prediction 'skateboarder'\n",
    "* Feature + 'start a skateboarder' , Prediction 'does'\n",
    "* Feature + 'start a skateboarder does' , Prediction 'a'\n",
    "* Feature + 'start a skateboarder does a', Predition 'trick'\n",
    "* Feature + 'start a skateboarder does a trick', Prediction 'on'\n",
    "* Feature + 'start a skateboarder does a trick on' , Prediction 'a'\n",
    "* Feature + 'start a skateboarder does a trick on a ', Prediction 'ramp'\n",
    "* Feature + 'start a skateboarder does a trick on a ramp' , Prediction 'end'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we need to\n",
    "* Add $start$ and $end$ with every caption .\n",
    "* Split the captions \n",
    "* Increase the features according to possible combinations of \"Feature+ caption\"\n",
    "* Tokenize the captions\n",
    "* Calculate maximum length of all captions\n",
    "* Pad the captions with zeros which are less than the maximum length\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Back to Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding $start$ and $end$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for desc_list in data_with_captions.values():\n",
    "    for d in desc_list:\n",
    "        desc_list[desc_list.index(d)] = 'start '+d+' end.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function for loading descriptions for each dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_desc(data):\n",
    "    desc =dict()\n",
    "    for image_name in data:\n",
    "        desc[image_name]=data_with_captions[image_name]\n",
    "    return desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_desc = load_desc(train)\n",
    "dev_desc = load_desc(dev)\n",
    "test_desc = load_desc(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = data_with_captions.values()\n",
    "texts = ' '.join([' '.join(text) for text in texts])\n",
    "texts_list = texts.split()\n",
    "vocab = sorted(set(texts_list))\n",
    "vocab_size=len(vocab)+1\n",
    "print('Vocabulary Size %d'%vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving Vocabulary for future use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(vocab,open('vocabulary.pkl','w'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenizing the words. You can use tokenizer class from ```keras```. But It didn't work very well in my case, I dont know why. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i2w = dict((i,c)for i,c in enumerate(vocab))\n",
    "w2i = dict((c,i)for i,c in enumerate(vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maximum length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length =max(max([[len(d.split()) for d in ls] for ls in train_desc.values()]))\n",
    "\n",
    "print(\"Maximum Length %d\"%(max_length))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function for Encoding the Captions according to self made tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_desc(description):\n",
    "#if True:\n",
    "    #description = train_desc\n",
    "    encoded_list = []\n",
    "    encoded_list_extend=[]\n",
    "    out_list=[]\n",
    "    encoded =dict()\n",
    "    for key in description.keys():\n",
    "        caps_encoded =[[w2i[word] for word in cap.split()] for cap in description[key]]\n",
    "        encoded[key] = caps_encoded\n",
    "        for cap in description[key]:\n",
    "            encoded_list.append([w2i[word] for word in cap.split()])\n",
    "    for ls in encoded_list:\n",
    "        j=1\n",
    "        for i in range(1,len(ls)):\n",
    "            word_encode= pad_sequences([ls[:i]],maxlen=max_length,padding='pre')\n",
    "            out = ls[i]\n",
    "            encoded_list_extend.append(word_encode.tolist())\n",
    "            out_list.append(out)\n",
    "            print('.'*j+'\\r'),\n",
    "            j=j+1\n",
    "    return encoded,encoded_list_extend,out_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_desc_encoded,train_desc_encoded_list,train_out = encode_desc(train_desc)\n",
    "dev_desc_encoded,dev_desc_encoded_list,dev_out = encode_desc(dev_desc)\n",
    "test_desc_encoded,test_desc_encoded_list,test_out = encode_desc(test_desc)\n",
    "\n",
    "train_desc_encoded\n",
    "\n",
    "train_desc_encoded_list\n",
    "\n",
    "train_desc_encoded_np = np.array(train_desc_encoded_list)\n",
    "dev_desc_encoded_np = np.array(dev_desc_encoded_list)\n",
    "test_desc_encoded_np = np.array(test_desc_encoded_list)\n",
    "print(\"Training array shape \"+str(train_desc_encoded_np.shape))\n",
    "print(\"Dev array shape \"+str(dev_desc_encoded_np.shape))\n",
    "print(\"Test array shape \"+str(test_desc_encoded_np.shape))\n",
    "\n",
    "train_desc_encoded_np = np.reshape(train_desc_encoded_np,(-1,train_desc_encoded_np.shape[2]))\n",
    "dev_desc_encoded_np = np.reshape(dev_desc_encoded_np,(-1,dev_desc_encoded_np.shape[2]))\n",
    "test_desc_encoded_np = np.reshape(test_desc_encoded_np,(-1,test_desc_encoded_np.shape[2]))\n",
    "print(\"Training array shape \"+str(train_desc_encoded_np.shape))\n",
    "print(\"Dev array shape \"+str(dev_desc_encoded_np.shape))\n",
    "print(\"Test array shape \"+str(test_desc_encoded_np.shape))\n",
    "\n",
    "train_out_np = np.array(train_out)\n",
    "dev_out_np = np.array(dev_out)\n",
    "test_out_np = np.array(test_out)\n",
    "print(\"Training Output array shape \"+str(train_out_np.shape))\n",
    "print(\"Dev Output array shape \"+str(dev_out_np.shape))\n",
    "print(\"Test Output array shape \"+str(test_out_np.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function for preparing features , again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_features(features_set,description_encoded):\n",
    "    x1=[]\n",
    "\n",
    "    #features_set = train_features_set\n",
    "    #description_encoded = train_desc_encoded\n",
    "    for key,values in features_set.items():\n",
    "        photo_descs = description_encoded[key]\n",
    "\n",
    "        j=0\n",
    "        for desc in photo_descs:\n",
    "            j=j+1\n",
    "            for i in range(1,len(desc)):\n",
    "\n",
    "                #in_seq = pad_sequences([desc[:i]],maxlen=max_length)[0]\n",
    "                #out_seq = np_utils.to_categorical(desc[i],num_classes=len(vocab)+1)[0]\n",
    "                x1.append(features_set[key][0])\n",
    "                #x2.append(in_seq)\n",
    "                #y.append(out_seq)\n",
    "\n",
    "    return x1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(full_model, to_file='model_new.png', show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It should give the following image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Model](model.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_x1)\n",
    "batch_size=1024\n",
    "#dev_x1 = dev_x1[:-batch_size]\n",
    "epochs = 20\n",
    "for epoch in range(epochs):\n",
    "    for i in range(0,len(train_x1),batch_size):\n",
    "        X1train = np.array(train_x1[i:i+batch_size])\n",
    "        X2train = train_desc_encoded_np[i:i+batch_size]\n",
    "        ytrain = train_out_np[i:i+batch_size]\n",
    "        X1test = np.array(dev_x1)\n",
    "        X2test = dev_desc_encoded_np\n",
    "        ytest = dev_out_np\n",
    "        full_model.fit([X1train, X2train], ytrain, verbose=0,batch_size=batch_size,validation_data=([X1test, X2test], ytest))\n",
    "    train_loss =full_model.evaluate(x=[X1train, X2train], y=ytrain,verbose=0)\n",
    "    Val_loss = full_model.evaluate(x=[X1test, X2test], y=ytest,verbose=0)\n",
    "    print(\"Epoch %d , Train Loss %f and Val Loss %f\"%(epoch,train_loss,Val_loss))\n",
    "\n",
    "#full_model.evaluate(x=[X1train,X2train],y=ytrain)\n",
    "full_model.evaluate(x=[X1test,X2test],y=ytest)\n",
    "\n",
    "model_json = full_model.to_json()\n",
    "with open(\"/output/Caption_model_VGG16.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "full_model.save_weights(\"/output/Caption_model_VGG16.h5\")\n",
    "print(\"Saved model to disk\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test The model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prev_model():\n",
    "    model= VGG16()\n",
    "    model.layers.pop()\n",
    "    model = Model(inputs=model.inputs, outputs=model.layers[-1].output)\n",
    "    #model.summary()\n",
    "    return model\n",
    "\n",
    "vocab = pickle.load(open('vocabulary.pkl'))\n",
    "vocab_size=len(vocab)+1\n",
    "print('Vocabulary Size %d'%vocab_size)\n",
    "i2w = dict((i,c)for i,c in enumerate(vocab))\n",
    "w2i = dict((c,i)for i,c in enumerate(vocab))\n",
    "\n",
    "max_length =35\n",
    "print(\"Maximum Length %d\"%(max_length))\n",
    "\n",
    "from keras.models import model_from_json\n",
    "json_file = open('output/Caption_model_VGG16.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "loaded_model.load_weights(\"output/Caption_model_VGG16.h5\")\n",
    "print(\"Loaded model from disk\")\n",
    "\n",
    "model =prev_model()\n",
    "def get_img_feature(filename,model=model):\n",
    "    \n",
    "    img = load_img(filename,target_size=(224,224))\n",
    "    img = img_to_array(img)\n",
    "    img = preprocess_input(img)\n",
    "    img = img.reshape((1,img.shape[0],img.shape[1],img.shape[2]))\n",
    "    feature = model.predict(img)\n",
    "    return feature\n",
    "\n",
    "test_dir ='Test Images'\n",
    "image_names =os.listdir(test_dir)\n",
    "\n",
    "for image_name in image_names:\n",
    "    \n",
    "    image_name_full = test_dir+'/'+image_name\n",
    "\n",
    "    features = get_img_feature(image_name_full)\n",
    "    in_text ='start'\n",
    "    in_text_encode = [w2i[in_text]]\n",
    "\n",
    "    seq = pad_sequences([in_text_encode],maxlen=max_length)\n",
    "    output =[]\n",
    "    print(image_name)\n",
    "    for i in range(max_length): \n",
    "        yoh = loaded_model.predict(x=[features,seq])\n",
    "        word_indice = np.argmax(yoh)\n",
    "        if i2w[word_indice]=='end.':\n",
    "            continue\n",
    "        else:\n",
    "        \n",
    "            output.append(i2w[word_indice])\n",
    "            seq = seq.tolist()[0]\n",
    "            seq.remove(seq[0])\n",
    "            seq.append(word_indice)\n",
    "            seq =np.array([seq])\n",
    "        \n",
    "    out_text =' '.join(output)\n",
    "    im = plt.imread(image_name_full)\n",
    "    plt.figure()\n",
    "    plt.imshow(im)\n",
    "    plt.title(out_text)\n",
    "    plt.savefig('Caption%d.jpg'%(image_names.index(image_name)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
